{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "To calculate the gradients, computing forward and backward are both practical, which is mainly because in essense they are based on the Chain Rule. But there are prons and cons of each method.\n",
    "- Forward\n",
    "  - prons:\n",
    "    - Occupy little amount of memory, for the fact that gradients accumulate into the next layer.\n",
    "  - cons:\n",
    "    - Every computation contains a whole flow going through each single nodes, which is a waste of time and also complex, being the reason why it is not widely used in Deep Learning.\n",
    "- Backward\n",
    "  - prons:\n",
    "    - To calculate the gradient of a variable, you only need to go back and look for nodes that include this variable, needless to go through the whole function.\n",
    "  - cons:\n",
    "    - It is imperative to store middle gradients, for the sake of being the bedrock of the calculation in former stages, leading to more consumption of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(4.0, requires_grad= True) # give the argument claiming the need of gradient calculation \n",
    "y = 2 * torch.dot(x, x)\n",
    "y.backward() # use backward propagation to compute the gradient\n",
    "x.grad # visit the gradient of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Annotation\n",
    "\n",
    "There's a warning saying *\"Only Tensors of floating point and complex dtype can require gradients\"*, out of my setting of the elements of x mistakenly being *\"x = torch.arange(**4**, requires_grad= True)\"* instead of using floating type ***4.0***. It is a detail worth attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14., 14., 14., 14.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's supposed to clear the gradient to compute the next time\n",
    "x.grad.zero_()\n",
    "\n",
    "# to fix some parameters to be constant\n",
    "y = torch.dot(x, x)\n",
    "u = y.detach() # let u be a constant instead of a function\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a slight insight of the pytorch, it might be partly reasonable to consider pytorch as a module or library similar to *pandas* or *numpy*, though it could work on more intricate problems and show capability and efficiency dealing with deep learning. However, as for now, it's fine to perceive it simply. As delving deeper, we will be able to understand it more correctly and precisely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
